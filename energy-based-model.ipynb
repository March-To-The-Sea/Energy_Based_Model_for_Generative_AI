{"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"","display_name":""},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"ebm_input = layers.Input(shape = (32, 32, 1))\nx = layers.Conv2D(16, kernel_size = 5, strides = 2, padding = \"same\",activation = activations.swish)(ebm_input)\nx = layers.Conv2D(32, kernel_size = 5, strides = 2, padding = \"same\",activation = activations.swish)(x)\nx = layers.Conv2D(64, kernel_size = 5, strides = 2, padding = \"same\",activation = activations.swish)(x)\nx = layers.Conv2D(64, kernel_size = 5, strides = 2, padding = \"same\",activation = activations.swish)(x)\nx = layers.Flatten()(x)\nx = layers.Dense(64, activation =activations.swish)(x)\nebm_output = layers.Dense(1)(x)\nmodel = models.Model(ebm_input, ebm_output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_samples(model, inp_imgs, steps, step_size, noise):\n    imgs_per_step = []\n    for _ in range(steps):\n        inp_imgs += tf.random.normal(inp_imgs.shape, mean = 0, stddev = noise)\n        inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1,0)\n        with tf.GradientTape() as tape:\n            tape.watch(inp_imgs)\n            out_score = -model(inp_imgs)\n        grads = tape.gradient(out_score, inp_imgs)\n        grads = tf.clip_by_value(grads, -0.03, 0.03)\n        inp_imgs += -step_size * grads\n        inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\n        return inp_imgs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Buffer:\n    def __init_(self, model):\n        super().__init__()\n        self.model = model\n        self.examples = [\n            tf.random.uniform(shape = (1, 32, 32, 1)) * 2 - 1\n            for _ in range(128)\n        ]\n    \n    def sample_new_exmps(self, steps, step_size, noise):\n        n_new = np.random.binomial(128, 0.05)\n        rand_imgs = (tf.random.uniform((n_new, 32, 32, 1)) * 2 - 1)\n        old_imgs = tf.concat(random.choices(self.examples, k = 128 - n_new), axis = 0)\n        inp_imgs = tf.concat([rand_imgs, old_imgs], axis = 0)\n        inp_imgs = generate_samples(self.model, inp_imgs, steps = steps, step_size = step_size, noise = noise)\n        self.examples = tf.split(inp_imgs, 128, axis = 0) + self.examples\n        self.examples = self.examples[:8192]\n        return inp_imgs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EBM(models.Model):\n    def __init__(self):\n        super(EBM, self).__init__()\n        self.model = model\n        self.buffer = Buffer(self.model)\n        self.alpha = 0.1\n        self.loss_metric = metrics.Mean(name = \"loss\")\n        self.reg_loss_metric = metrics.Mean(name = \"log\")\n        self.cdiv_loss_metric = metrics.Mean(name = \"cdiv\")\n        self.real_out_metric = metrics.Mean(name = \"real\")\n        self.fake_out_metric = metrics.Mean(name = \"fake\")\n    \n    @property\n    def metrics(self):\n        return [\n            self.loss_metric,\n            self.reg_loss_metric,\n            self.cdiv_loss_metric ,\n            self.real_out_metric,\n            self.fake_out_metric\n        ]\n    \n    def train_step(self, real_imgs):\n        real_imgs += tf.random.normal(\n            shape = tf.shape(real_imgs),  mean = 0, stddev = 0.005\n        )\n        real_imgs = tf.clip_by_value(real_imgs, -1.0, 1.0)\n        fake_imgs = self.buffer.sample_new_exmps(\n            steps = 60, step_size = 10, noise = 0.005\n        )\n        inp_imgs = tf.concat([real_imgs, fake_imgs], axis = 0)\n        with tf.GradientTApe() as training_tape:\n            real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis = 0)\n            cdiv_loss = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(\n                real_out, axis = 0\n            )\n            reg-loss = self.alpha * tf.reduce_mean(\n                real-out ** 2 + fake_out ** 2, axis = 0\n            )\n            loss = reg_loss + cdiv_loss\n        grads = training_tape.gradient(loss, self.model.trainable_variables)\n        self.optimizer.apply_gradients(\n            zip(grads, self.model.trainable_variables)\n        )\n        self.loss_metric.update_state(loss)\n        self.reg_loss_metric.update_state(reg_loss)\n        self.cdiv_loss_metric.update_state(cdiv_loss)\n        self.real_out_metric.update_state(tf.reduce_man(real_out, axis = 0))\n        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis = 0))\n        return {m.name: m.result for m in self.metrics}\n    \n    def test_step(self, real_imgs):\n        batch_size = real_imgs.shape[0]\n        fake_imgs = tf.random.uniform((batch_size, 32, 32, 1)) * 2 - 1\n        inp_imgs = tf.concat([real_imgs, fake_imgs], axis = 0)\n        real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis = 0)\n        cdiv = tf.reduce_mean(fake_out, axis = 0) - tf.reduce_mean(real_out, axis = 0)\n        self.cdiv_loss_metric.update_state(cdiv)\n        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis = 0))\n        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis = 0))\n        return {m.name: m.result() for m in self.metrics[2:]}\n    \n    ebm = EBM()\n    ebm.compile(optimizer = optimizers.Adam(learning_rate = 0.0001), run_eagerly = True)\n    ebm.fit(x_train, epochs = 60, validation_data = x_test,)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_imgs = np.random.uniform(size = (10, 32, 32, 1)) * 2 - 1\n\ngen_img = generate_samples(\n ebm.model,\n start_imgs,\n steps=1000,\n step_size=10,\n noise = 0.005,\n return_img_per_step=True,\n)","metadata":{},"execution_count":null,"outputs":[]}]}